{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post \n",
    "title: Computing Bias  \n",
    "description: Computing Bias \n",
    "courses: {csp: {week: 1} }\n",
    "comments: true \n",
    "sticky_rank: 1 \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "\n",
    "Computing bias is unfair preferences. \n",
    "Netflix is known fgor suggesting shows and movies based on what you've watched before, but it often alters its recommendations. \n",
    "Virtual assistants have female voices because women are viewed as helpers or assistants. \n",
    "HP Camera Incident: Racist camera bc it couldn't detect people with the darker skin complexion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack 1 \n",
    "\n",
    "A software that demonstrates bias is Instagram because he company has been accused for being biased against influencers and artists whose content doesn't align with specific values (usually, posts that are sexual or political) or whose views align with or promote LGBTQIA per BIPOC philosophies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack 2 \n",
    "\n",
    "One time technology didn't work well was when I was trying to ask ChatGPT a math question and it kept giving me the wrong answer. It made me feel like I couldn't trust AI as much as I thought I could. One way technology could be more inclusive would be if technology were able to adapt to people with certain disabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack 3 \n",
    "\n",
    "Algorithms for fitness goals or health recommendations might be designed with \"average\" users in mind, ignoring variations in health needs, fitness levels, or cultural practices. Users might receive inappropriate goals, such as overly aggressive calorie deficits or step counts that are unachievable for people with mobility challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Hacks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One digital tool I use regualrly is Youtube. YouTube's recommendation system often suggests videos based on my watch history, subscriptions, and liked content. But there are some noticeable patterns in the ceommendations that sugguest that they are biased. For example, the platform tends to prioritize videos that are from popular vreators instead of smaller channels. Also, the content is usually influenced by previous viewing habits, which can lead to the user only recieving content that they are used to watching. The cause of this bias is likely rooted in Youtube's algorithm design, which heavily relies on getting user data such as watch history, likes, and engagement. While this approach aims to enhance user experience by suggesting content that aligns with the user's' interests, it reinforces existing preferences and doesn't offer the user anything new. Additionally, the algorithm may not be trained to prioritize diversity in recommendations, which creates a division with creators on the platform too. To reduce the bias and make the platform more inclusive, Youtube developers could refine the recommendation algorithm to incorporate a wider range of content types. For example, they coudl introduce features that intentionally highlight underrepresented creators, diverse perspectives, or content in different languages. ALso, implementing more diverse testing in the algorithm's design phase wuold ensure that it better serves different user demographics and types. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
